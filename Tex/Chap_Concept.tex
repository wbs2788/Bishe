\chapter{基于潜空间偏好优化的视频概念擦除方法}\label{chap:concept}

\section{引言}\label{sec:introduction}

在前一章中，构建了基于大语言模型的前置检测机制，有效从输入端拦截了恶意提示词的注入。然而，这种外挂式的防御手段并未改变生成模型本身的参数分布，模型内部依然存储着诸如版权形象、暴力行为模式或社会偏见等有害知识。一旦攻击者通过复杂的越狱手段绕过前置检测，或者在非对抗环境下无意触发了相关特征，模型仍会生成违规内容。因此，构建内生安全机制，即在模型参数层面直接剔除特定有害概念而不损害通用生成能力，构成了视频生成安全体系的第二道防线。这一过程在学术界被称为机器遗忘或概念擦除，其核心目标是实现对模型内部知识的精准外科手术式修正。

尽管针对静态图像扩散模型的概念擦除研究已取得一定进展，但将其直接迁移至视频生成任务面临着严峻的理论与工程挑战。首先是计算资源的瓶颈问题。现有的图像擦除方法通常依赖于像素级的监督信号，需要反复调用变分自编码器进行图像的解码与编码。对于视频生成而言，高维度的时空数据使得这一过程的显存占用与计算开销呈指数级增长，导致在消费级硬件上进行模型微调几乎成为不可能。其次是时空一致性的维持难题。视频生成不仅仅是连续帧的堆叠，更涉及复杂的运动逻辑与因果依赖。简单的逐帧擦除策略往往会破坏帧间的连贯性，导致生成的视频出现画面闪烁、物体形变或运动轨迹断裂，严重损害了视频的可用性。

针对上述挑战，本章提出了一种基于潜空间偏好优化的视频概念擦除方法。该方法摒弃了传统的像素级监督范式，转而利用扩散模型去噪轨迹与生成概率之间的数学映射关系，直接在低维潜空间内进行对抗性优化。通过推导扩散模型中的直接偏好优化理论，将概念擦除任务转化为一个无需显式奖励模型的分类问题，从而规避了高昂的视频解码开销。进一步地，针对视频生成的时空特性，设计了时空-语义对齐机制。该机制通过约束模型注意力模块中的键值上下文，强迫模型在移除目标概念的同时锁定视频的时空骨架，确保了擦除后的视频在物理布局与运动逻辑上与原视频保持高度一致。本章将详细阐述这一框架的理论推导、算法实现及实验验证，旨在为高维视频生成模型提供一种高效、稳定且精准的安全对齐方案。

\section{浅空间内的直接偏好优化}\label{sec:direct_preference_optimization}
\subsection{问题定义}\label{subsec:problem_definition}

在生成式模型的安全对齐任务中，传统的监督微调（SFT）往往难以精确地将人类复杂的价值观（如“移除版权角色但保留画风”）转化为可优化的损失函数。
因此，引入基于人类反馈的强化学习（Reinforcement Learning from Human Feedback, RLHF）成为了主流范式。
然而，标准的基于人类反馈的强化学习流程通常需要训练一个独立的奖励模型（Reward Model），并使用强化学习算法进行在线优化，这在计算成本高昂且训练不稳定的视频生成任务中极难落地。
直接偏好优化（Direct Preference Optimization, DPO）的提出，通过严谨的数学推导将强化学习问题转化为一个无需显式奖励模型的分类问题，为这一难题提供了优雅的闭式解。

本节将从标准基于人类反馈的强化学习的目标函数出发，推导直接偏好优化算法的理论根基。

\subsubsection{基于人类反馈的强化学习的目标函数与最优策略}

在 RLHF 框架中，目标是寻找一个最优策略 $\pi_{\theta}$（即生成的模型），使其在最大化预期奖励的同时，保持与原始参考模型 $\pi_{\text{ref}}$（Reference Model）的分布接近，以防止模型在优化过程中发生模式崩塌或过度偏离预训练的语言或视觉分布。
该优化问题可以形式化为带有 KL 散度约束的奖励最大化问题：
\begin{equation}
\max_{\pi} \mathcal{J}(\pi) = \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi(\cdot|x)} \left[ r^*(x, y) - \beta \mathbb{D}_{\text{KL}}(\pi(\cdot|x) \Vert \pi_{\text{ref}}(\cdot|x)) \right].
\end{equation}
其中：$x$ 为输入提示词（Prompt），$y$ 为生成的视频内容；$r^*(x, y)$ 为真实但未知的奖励函数，反映了人类对生成结果的偏好（例如：不含敏感概念的内容奖励高）；$\beta$ 为 KL 散度惩罚系数，用于控制模型偏离参考模型的程度；$\mathbb{D}_{\text{KL}}$ 为 Kullback-Leibler 散度。
根据凸优化理论，上述目标函数在无参数约束的函数空间内存在唯一的解析解（Closed-form Solution）。通过变分法求解泛函极值，可以得到最优策略 $\pi^*$ 的形式为：
\begin{equation}\label{eq:optimal_policy}
\pi^*(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp \left( \frac{1}{\beta} r^*(x, y) \right).
\end{equation}
其中，$Z(x) = \sum_y \pi_{\text{ref}}(y|x) \exp(\frac{1}{\beta} r^*(x, y))$ 为配分函数（Partition Function），用于保证概率归一化。

\subsubsection{奖励函数的重参数化}

公式~\ref{eq:optimal_policy}虽然给出了最优策略的形式，但其中包含未知的奖励函数 $r^*(x, y)$ 和难以计算的配分函数 $Z(x)$。
DPO 的核心洞察在于：既然最优策略与奖励函数存在一一映射关系，反之可以用最优策略来表示奖励函数。

对公式~\ref{eq:optimal_policy} 两边取对数并重排各项，可得：
\begin{equation}\label{eq:reward_function}
r^*(x, y) = \beta \log \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x).
\end{equation}

这一步将抽象的奖励具体化为最优策略与参考策略的对数似然比。

\subsubsection{直接偏好优化}

在偏好学习中，通常假设人类的偏好遵循 Bradley-Terry (BT) 模型。
给定两个生成样本 $y_w$（胜者，Preferred）和 $y_l$（败者，Dispreferred），人类偏好 $y_w \succ y_l$ 的概率由两者奖励差值的 Sigmoid 函数决定：
\begin{equation}\label{eq:bt_model}
p(y_w \succ y_l | x) = \sigma(r^*(x, y_w) - r^*(x, y_l)),
\end{equation}
其中 $\sigma(z) = \frac{1}{1 + e^{-z}}$ 为 Logistic 函数。

将公式~\ref{eq:reward_function} 代入公式~\ref{eq:bt_model} 中的奖励差值项 $r^*(x, y_w) - r^*(x, y_l)$。
此时，配分函数项 $\beta \log Z(x)$ 由于与 $y$ 无关，在相减过程中被抵消：
\begin{equation}\label{eq:reward_difference}
r^*(x, y_w) - r^*(x, y_l) = \beta \log \frac{\pi^*(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi^*(y_l|x)}{\pi_{\text{ref}}(y_l|x)}.
\end{equation}

这一结果表明，人类的偏好概率可以直接由策略模型 $\pi^*$ 来表达，而无需显式建模奖励函数 $r^*$。
因此，将参数化的模型 $\pi_\theta$ 视为对最优策略 $\pi^*$ 的逼近，通过最大化偏好数据的对数似然，即可直接优化策略模型。
最终，DPO 的目标函数定义为：
\begin{equation}\label{eq:dpo_objective}
\mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right].
\end{equation}

通过上述推导，证明了直接偏好优化实际上是在隐式地优化一个满足 KL 约束的奖励最大化问题。
该方法避免了训练奖励模型带来的偏差传递和强化学习算法中复杂的超参数调优，具有训练稳定、收敛速度快且易于实现的理论优势。
这为高维视频潜空间中的概念擦除提供了坚实的数学基础。

\subsection{扩散模型中的直接偏好优化}\label{subsec:dpo_diffusion}
直接偏好优化最初是为大语言模型（LLMs）等自回归生成模型设计的。
在大语言模型的语境下，模型输出被定义为离散 Token 序列的联合概率分布，其对数似然 $\log \pi_\theta(y|x)$ 可以通过链式法则精确计算为所有 Token 预测概率的对数之和。
然而，直接将 DPO 范式迁移至视频扩散模型面临着根本性的数学挑战。

\subsubsection{从“下一个令牌预测”到“去噪预测”的范式转换}

视频扩散模型（如 Wan-2.1）是在连续的高维潜空间内通过去噪过程生成数据，而非离散的 Token 预测。
对于扩散模型而言，直接计算生成样本 $V$ 的精确对数似然 $\log \pi_\theta(V|c)$ 是不可解的，因为这需要对所有可能的反向去噪轨迹进行积分。
为了解决这一难题，基于扩散模型的变分下界（Evidence Lower Bound, ELBO）理论进行适配。
根据去噪分数匹配（Denoising Score Matching）理论，扩散模型在时间步 $t$ 的去噪均方误差（MSE）可以视为该时刻负对数似然的变分上界。
具体而言，对于给定条件 $c$ 和加噪潜变量 $z_t$，模型的对数概率与去噪误差存在如下近似关系：
\begin{equation}
    \log \pi_\theta(V|c) \approx - \mathbb{E}_{t, \epsilon} \left[ \omega(t) \| \epsilon - \epsilon_\theta(z_t, t, c) \|_2^2 \right] + C,
\end{equation}
其中，$\epsilon$ 为高斯噪声，$\epsilon_\theta$ 为模型预测噪声，$\omega(t)$ 为时间加权系数，$C$ 为常数项。

这一关系揭示了 DPO 适配的核心机制：模型生成概率的对数比（Log-ratio）可以转化为去噪误差的差值。
即，如果模型 $\pi_\theta$ 比参考模型 $\pi_{\text{ref}}$ 更倾向于生成样本 $V$，意味着 $\pi_\theta$ 在重建 $V$ 时的去噪误差应小于 $\pi_{\text{ref}}$ 的去噪误差。

\subsubsection{扩散偏好损失函数的定义}

基于上述理论，将 DPO 的优化目标从概率空间映射到去噪误差空间。
对于成对的视频样本——优选样本 $V_w$（Winner，即去除概念后的视频）和厌恶样本 $V_l$（Loser，即包含概念的视频），将扩散模型的偏好损失函数定义为：
\begin{equation}\label{eq:dpo_loss}
\mathcal{L}_{DPO} = -\log \sigma \left( \beta \left( \log \frac{\pi_\theta(V_w|c)}{\pi_{ref}(V_w|c)} - \log \frac{\pi_\theta(V_l|c)}{\pi_{ref}(V_l|c)} \right) \right).
\end{equation}
在实际训练中，公式中的对数概率项由去噪均方误差误差项替代。具体地，定义隐式奖励差值（Implicit Reward Margin）$\Delta$ 为：
\begin{equation}\label{eq:implicit_reward_margin}
\Delta = \underbrace{\left( \| \epsilon - \epsilon_\theta(z_t, V_w) \|^2 - \| \epsilon - \epsilon_{\text{ref}}(z_t, V_w) \|^2 \right)}_{\text{优选样本的相对改善}} - \underbrace{\left( \| \epsilon - \epsilon_\theta(z_t, V_l) \|^2 - \| \epsilon - \epsilon_{\text{ref}}(z_t, V_l) \|^2 \right)}_{\text{厌恶样本的相对改善}}.
\end{equation}
公式~\ref{eq:dpo_loss}的物理意义极为直观，$\pi_\theta(V_w|c)$ 与 $\pi_\theta(V_l|c)$分别代表当前模型生成优选视频和厌恶视频的概率（通过去噪误差负相关度量）。$\pi_{\text{ref}}$：作为正则化项（Reference），防止模型在优化过程中过度偏离预训练的先验分布，保证生成视频的结构合理性。$\beta$：温度系数（超参数），控制偏好优化的强度。通过最小化该损失函数，迫使模型在潜空间中调整去噪轨迹：降低优选样本 $V_w$（无概念）的去噪误差，同时增大厌恶样本 $V_l$（有概念）的去噪误差。这实现了在不进行像素级解码的情况下，直接在模型参数层面擦除特定的语义概念。

通过最小化该损失函数，迫使模型在潜空间中调整去噪轨迹：降低优选样本 $V_w$（无概念）的去噪误差，同时增大厌恶样本 $V_l$（有概念）的去噪误差。这实现了在不进行像素级解码的情况下，直接在模型参数层面擦除特定的语义概念。

\subsection{浅空间免解码优化策略}\label{subsec:free_form_optimization}

在传统的图像或视频概念擦除方法（如 ESD～\cite{gandikota2023erasing}, UCE~\cite{Gandikota2023UnifiedCE}）中，通常需要将潜变量解码为像素图像，计算视觉损失（如物体检测 loss 或身份 ID loss），再将梯度反向传播回扩散模型。
对于视频生成任务而言，这种“解码-计算-回传”的路径极其昂贵。
视频变分自编码器（Video VAE）通常包含复杂的 3D 卷积层，单次解码的显存占用和计算开销是潜空间推理的数倍，且极易导致梯度在长时序传导中消失或爆炸。

针对这一痛点，本文提出了潜空间免解码优化策略。
概念擦除本质上是一个轨迹级的分布重塑问题，而非单帧的像素级修复。
因此直接在 DiT 的潜空间内，通过对比去噪轨迹的优劣来更新模型，完全避免了高昂的视频解码过程。

为了实现稳定的偏好优化，本研究构建了一个教师-学生对齐架构。其中，教师模型（Teacher Model, $\theta_0$）由原始的预训练 Wan-2.1 模型初始化，其参数在训练全过程中保持冻结状态。教师模型的核心作用是作为参考锚点（Reference Anchor），提供原始的去噪能力基准，旨在防止学生模型在执行概念擦除任务时破坏视频的基础生成质量，诸如光影渲染与物理规律的合理性。与之相对，学生模型（Student Model, $\theta$）作为待优化的目标网络，采取了参数高效的微调策略。具体而言，Wan-2.1 的主体参数被冻结，仅在 Diffusion Transformer (DiT) 注意力模块的查询（Query）与键（Key）投影层中插入了秩为 64 的低秩自适应适配器进行定向更新。

\subsubsection{基于均方误差差分的偏好边际}
给定一个在时间步 $t$ 共享的噪声潜变量 $\mathbf{z}_t \in \mathbb{R}^{(1+T/4)\times H/8\times W/8\times C}$，分别输入优选提示词（Concept-free, $x^-$）和厌恶提示词（Concept-bearing, $x^+$）。
学生模型与教师模型分别输出四个噪声预测值：
\begin{align}
    \epsilon^{-}{\text{stu}}    &= \epsilon_{\theta}\bigl(\mathbf{z}_t,t,x^{-}\bigr), &\epsilon^{+}{\text{stu}}    &= \epsilon_{\theta}\bigl(\mathbf{z}_t,t,x^{+}\bigr), \\
    \epsilon^{-}{\text{teach}} &= \epsilon_{\theta_{0}}\bigl(\mathbf{z}_t,t,x^{-}\bigr), &\epsilon^{+}{\text{teach}} &= \epsilon_{\theta_{0}}\bigl(\mathbf{z}_t,t,x^{+}\bigr).
\end{align}

根据~\ref{subsec:dpo_diffusion}节的理论，扩散模型的负对数似然与去噪均方误差成正比。因此，可以通过计算预测噪声与真实高斯噪声 $\epsilon$ 之间的均方误差来量化模型对当前生成的置信度或偏好度：
\begin{align}
    \ell_{\text{stu}}^{+}    &= \Vert \epsilon - \epsilon^{+}{\text{stu}}    \Vert^2, &\ell{\text{stu}}^{-}    &= \Vert \epsilon - \epsilon^{-}{\text{stu}}    \Vert^2, \\
    \ell{\text{teach}}^{+} &= \Vert \epsilon - \epsilon^{+}{\text{teach}} \Vert^2, &\ell{\text{teach}}^{-} &= \Vert \epsilon - \epsilon^{-}{\text{teach}} \Vert^2.
\end{align}

为了消除不同噪声水平下的尺度差异，引入教师归一化边际 $m$。
它衡量了学生模型相对于原始教师模型在去噪误差上的变化量：
\begin{equation}
    m^{+}=\ell_{\text{stu}}^{+}-\ell_{\text{teach}}^{+},\quad m^{-}=\ell_{\text{stu}}^{-}-\ell_{\text{teach}}^{-}.
\end{equation}

最终，定义总的偏好差值 $\Delta$ 为：
\begin{equation}
    \Delta = m^{+} - m^{-}.
\end{equation}

上述推导中定义的偏好差值 $\Delta$ 具有明确的物理阐释。具体而言，$m^-$ 的负值表征学生模型在优选提示词下的重建误差低于教师模型，意味着模型已成功习得更优的无概念视频生成能力；与之对应，$m^+$ 的正值则反映学生模型在厌恶提示词下的误差高于教师模型，表明其生成特定概念视频的能力发生了预期的退化。基于此，算法的优化目标被设定为最小化 $\Delta$ 值，通过驱动该指标向负值偏移，能够在增强无概念生成质量的同时有效抑制有概念内容的产生。

将 $\Delta$ 代入 Sigmoid 函数，得到最终的 Latent DPO 损失函数：
\begin{equation}
    \mathcal{L}_{\mathrm{DPO}} = -\log \sigma\Bigl(-\frac{\beta}{2}\Delta\Bigr),
\end{equation}
其中 $\beta$（取值为 0.1）为逆温度系数，用于控制偏好分布的锐度。该损失函数通过简单的二元分类逻辑，驱动模型在潜空间轨迹上发生偏转，使其在面对 $x^+$ 时产生高去噪误差（从而生成模糊或无效内容），而在面对 $x^-$ 时保持或提升生成质量。

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{Img/diffusiondpo.png}
    \bicaption{DPO 扩散偏好损失函数示意图}{Illustration of the DPO diffusion preference loss function.}
    \label{fig:dpo_diffusion}
\end{figure}

如图~\ref{fig:dpo_diffusion}所示，该示意图直观地展示了通过计算学生模型与教师模型在优选和厌恶样本上的去噪误差差分，进而构建偏好损失并引导模型优化的完整流程。


\section{时空-语义对齐损失函数}\label{sec:spatiotemporal_semantic_alignment}

在将概念擦除技术从静态图像迁移至动态视频的过程中，最棘手的挑战在于维持时空一致性（Spatiotemporal Consistency）。
图像擦除方法（如 ESD, UCE）只关注图像内的语义修改，若直接将其应用于视频的每一帧，极易导致帧间闪烁（Flickering）或物体运动轨迹的崩塌，无法关注到视频的时序一致性。

为了解决这一问题，本文提出了一种专为视频 DiT 架构设计的时空-语义对齐（Spatiotemporal-Semantic Alignment, STSA）损失函数。
该机制的核心思想是：在擦除目标概念（如米老鼠的身份特征）的同时，强制模型在潜空间内的注意力上下文（Attention Context）保持不变，从而锁定的视频的物理布局与运动逻辑。

\subsection{DiT 架构中的时空注意力机制}\label{subsec:spatiotemporal_attention}

为了理解时空-语义对齐损失的作用机理，首先需要剖析 Wan-2.1 模型所采用的 Diffusion Transformer (DiT) 架构。
与传统基于 3D-UNet 的视频生成模型不同，DiT 摒弃了时空分离的卷积归纳偏置，采用了全注意力的序列建模方式。

在 Wan-2.1 的 DiT 模块中，输入的含噪视频潜变量 $z_t$ 首先经历三维令牌化处理。算法将潜变量切分为互不重叠的三维补丁，并将其展平为一维序列，其序列长度 $N$ 由时间帧数、空间高度与宽度维度的乘积共同决定。随后，这些令牌在注意力层中被投影为查询、键与值向量，并进入全时空注意力编码阶段。区别于仅在空间维度计算的图像生成模型，视频 DiT 的注意力机制在全时空范围内同时运作。其中，空间编码负责捕捉帧内的纹理细节与拓扑结构，例如确立头部与身体的相对位置；时间编码则专注于捕捉帧间的运动轨迹与因果逻辑，例如建立相邻帧间肢体动作的连续性关联。

这种全时空注意力机制（Spatiotemporal Attention）使得模型能够生成连贯的动态视频，但也意味着一旦某个 Token 的语义被粗暴修改，可能会引发蝴蝶效应，破坏整个序列的时空结构。

\subsection{交叉注意力上下文的物理意义}

为了精准控制擦除过程，关注 DiT 内部的交叉注意力（Cross-Attention）或自注意力模块中的上下文矩阵（Context Matrix）。
根据 Wan-2.1 的架构特性，将注意力上下文 $C$ 定义为基于键（Key）的加权聚合：
\begin{equation}
    A = \text{Softmax}\left(\frac{K Q^\top}{\sqrt{d}}\right) \in \mathbb{R}^{N \times N},
\end{equation}
注意力图 $A$（Attention Map）编码了视频的结构与布局。
例如，它决定了第 5 帧的人物应该关注第 4 帧的相同位置（时间连续性）以及周围的背景像素（空间一致性）。
查询 $Q$（Query Features）编码了该位置的基础语义内容。

\begin{equation}
    C = A Q \in \mathbb{R}^{N \times d},    
\end{equation}
式中矩阵 $C$ 的每一行 $C_i$ 具有明确的物理含义——它代表了在特定的时空位置 $i$，模型为了生成当前特征，从全局序列中聚合了哪些语义信息。

如果在概念擦除过程中，上下文矩阵 $C$ 发生了剧烈变化，这通常意味着视频生成过程遭遇了显著的结构性或语义性损伤。具体而言，这种变化主要指向两种潜在的负面后果：其一为时空结构的破坏，即注意力图 $A$ 的改变导致了物体形变、位置漂移或背景错位，破坏了视频的物理连贯性；其二为非目标语义的丢失，即聚合特征 $Q$ 的变动引发了非目标属性的漂移，例如画风突变、光影不一致或无关物体的误伤。

因此，保持 $C$ 的稳定性，就是保持视频骨架与背景的稳定性。

\subsection{时空-语义对齐损失}\label{subsec:spatiotemporal_semantic_alignment_loss}
基于上述理论洞察，本研究构建了时空-语义对齐损失函数。在潜空间偏好优化的训练进程中，针对同一对输入提示词与共享的噪声潜变量，模型生成无概念视频时的内部注意力流向应当与生成有概念视频时保持严格的同构性。该损失函数的形式化定义如下：

\begin{equation}\label{eq:spatiotemporal_semantic_alignment_loss}
\mathcal{L}*{STSA} = \frac{1}{L \cdot N} \sum*{l=1}^{L} \Vert C^{(l)}(z_t, x^+) - C^{(l)}(z_t, x^-)\Vert_2^2
\end{equation}

其中， 代表 DiT 模型的网络层数， 则表示时空令牌的总量。

该损失函数的作用机理主要体现在三个维度。首先，通过最小化上下文矩阵的差异，算法约束了视频的布局与时序演化。这引导学生模型在处理无概念提示词时，能够复用由有概念提示词建立的时空注意力模式。具体而言，若原视频中主体在画面特定区域进行定向移动，擦除后的视频中生成的新角色须遵循完全一致的运动轨迹与节奏。其次，该机制实现了概念与结构的有效解耦。潜空间偏好优化损失函数主要驱动模型修改值投影矩阵，即改变生成的具体语义内容以移除目标概念。与之配合，时空-语义对齐损失函数则负责锁定注意力图与查询聚合状态，从而维持生成内容的物理位置与运动逻辑不变。最后，该损失函数发挥了关键的正则化作用。它有效抑制了低秩自适应参数在优化过程中的过拟合倾向，避免了因偏好优化信号过强而导致的模式崩塌，从而确保了擦除操作在维持视频画质前提下的精准度。

通过结合 $\mathcal{L}_{DPO}$ 和 $\mathcal{L}_{STSA}$，实现了在移除有害概念的同时，完美保留视频的时空一致性。

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{Img/stsa.png}
    \bicaption{时空-语义对齐损失函数示意图}{Illustration of the spatial-temporal semantic alignment loss function}
    \label{fig:stsa_loss}
\end{figure}

如图~\ref{fig:stsa_loss}所示，该示意图展示了时空-语义对齐损失如何通过约束不同提示词下的注意力上下文，确保视频在概念擦除过程中的结构稳定性。

\section{算法实现与训练策略}

本节将深入阐述 Latent-DPO 算法在 Wan-2.1 视频扩散模型上的具体实现细节。
针对视频生成模型参数量巨大且计算开销高昂的特点，设计了基于低秩自适应的微调策略，并构建了多目标联合优化框架，在保证算法有效性的同时，显著降低了训练资源需求。

\subsection{基于低秩自适应的微调机制}

在第 3.2.2 节中，介绍了低秩自适应适配器的基本原理。然而，与 LLM 的文本生成任务不同，针对 Wan-2.1 视频扩散模型的概念擦除任务涉及对高维视觉流形的重塑。因此，本节针对视频生成的特性，重新设计了低秩自适应适配器的注入位置与容量配置。

与第三章针对 LLM 微调 $W_q, W_v$ 不同，在视频 DiT 的概念擦除中，将低秩自适应适配器精确注入到自注意力与交叉注意力模块的查询（Query, $W_q$）与键（Key, $W_k$）投影层。
该差异化设计的核心逻辑在于重塑模型的注意力导向，而非直接干预视觉内容的生成。
在自注意力机制中，查询矩阵与键矩阵的交互直接决定了注意力图的分布。
这一分布本质上构成了信息的路由策略，指导模型在时空维度上如何分配关注点。
本研究通过微调这两个投影层，旨在精准阻断模型对特定敏感概念的特征聚合路径，而非破坏其基础的纹理生成能力。
与之相对，值投影层通常负责编码具体的视觉纹理与色彩信息。
将该层参数保持冻结状态至关重要，因为这能够最大程度地保留预训练模型既有的高质量图像生成能力，有效防止因过度拟合优化目标而引发的视频画质崩塌或风格漂移。

针对模型容量权衡与秩的选择问题，鉴于视频生成所涉潜空间维度显著高于文本模态，加之概念擦除任务本质上要求模型抑制其预训练阶段形成的强先验分布，这对微调模块的参数容量提出了更苛刻的要求。基于第4.5节的消融实验结果，本研究最终将低秩矩阵的秩设定为64。这一数值显著高于前文检测任务中采用的配置，反映了生成任务对特征重塑能力的更高依赖。

秩的选择实质上是模型表达能力与泛化边界之间的博弈。一方面，若秩的设置过低，有限的参数空间将难以精确拟合复杂的概念擦除边界，导致欠拟合现象，即无法彻底剥离目标特征。另一方面，若盲目增加秩的维度，过量的可训练参数会显著增加过拟合风险，进而可能导致模型在非目标概念上产生非预期的语义漂移。实验表明，秩为64的配置在擦除效能与模型稳定性之间取得了最佳平衡。此时可训练参数量约为2亿，相对于Wan-2.1模型13亿的主体参数规模，该方案在保证介入深度的同时依然维持了较高的参数效率。

$r=64$ 在擦除效能与模型稳定性之间取得了最佳平衡，此时可训练参数量约为 200M，相对于 Wan-2.1 1.3B 的主体参数依然保持了较高的参数效率。

\subsection{多目标联合优化函数}

为兼顾有害概念的精确剔除与视频时空连贯性的维持，本研究将潜空间偏好损失  与时空-语义对齐损失  进行融合，构建了多目标联合优化函数：

\begin{equation}
\mathcal{L}_{\text{total}} = \lambda_{DPO} \mathcal{L}_{DPO} + \lambda_{STSA} \mathcal{L}_{STSA}
\end{equation}

在多目标优化体系中，不同损失项之间的梯度量级平衡直接决定了模型的收敛行为。本研究将  设定为 1.0，确立其作为主导优化方向的地位，旨在强力驱动模型参数向概念遗忘区域收敛。与此同时， 被设定为 0.1。这一非对称权重的设定源于对训练动态的实验观测，数据显示注意力上下文差异的 L2 范数在数值量级上通常比 DPO 的对数几率损失高出一个数量级。引入 0.1 的权重系数能够有效地将两项损失规范化至同一数值尺度。该策略避免了辅助性的对齐损失因数值过大而主导梯度更新方向，从而确保了概念擦除任务的有效执行。

\subsection{实验设置与实施细节}

为了全面且客观地评估所提方法的有效性，本节首先明确实验所依据的数据基础、对比基准的选择逻辑以及具体的训练实施细节。所有实验评估均基于本文第二章构建的 VCE-24K 基准数据集。该数据集包含两万四千对经过严格筛选的高质量视频样本，覆盖了版权卡通角色与现实公众人物等多种高风险概念。为了严格测试模型对未知概念的泛化能力而非单纯的样本记忆，数据划分策略采用了留出法。具体而言，实验选取了一百二十个在训练阶段从未出现过的提示词作为测试集。这些留出提示词涵盖了多样化的场景描述与动作指令，确保了评估结果能够真实反映模型在面对新颖用户请求时的擦除效能。

在对比基准的选择上，本研究充分考虑了视频生成任务的独特性与计算约束。尽管图像扩散模型领域已存在如 ESD 与 UCE 等概念擦除算法，但本研究并未将其作为主要的定量对比基线。其核心原因在于，这些方法依赖于像素级的监督信号与逐帧解码机制。若将其直接迁移至高维度的视频生成任务，不仅会导致显存溢出等不可接受的计算成本，更无法保证视频帧间的时序连贯性，极易产生画面闪烁现象。因此，对比分析主要围绕内部基线与消融变体展开。原始且未修改权重的 Wan-2.1 模型被设定为“无擦除”组，旨在提供视频生成质量的参考上限。基于分支生成的推理策略被设定为“免训练”组，这代表了仅通过推理阶段干预所能达到的性能边界。此外，为了验证潜空间偏好优化与时空-语义对齐损失的独立贡献，实验还设置了仅保留单一模块的消融变体。

针对具体的训练实施细节，所有实验均在配备了八张 NVIDIA H100 图形处理器的计算集群上完成。基座模型选用 Wan-2.1-1.3B 文生视频模型。为了实现参数高效的微调，低秩自适应适配器被精确注入到注意力模块的查询与键投影层中，其秩超参数被设定为 64。优化过程采用 AdamW 优化器，学习率固定为 1e-3。单卡批次大小设定为 1，配合步数为 2 的梯度累积策略，以确保训练过程的稳定性。总训练时长覆盖了一百个轮次。在推理采样阶段，无分类器引导比例被固定为 5，以在文本对齐度与生成多样性之间取得平衡。通过上述标准化的实验配置，确保了实验结果的可复现性与严谨性。


\subsection{评价指标体系}

为了全面且客观地衡量概念擦除算法的综合性能，本研究构建了涵盖安全性有效性与实用性质量的双重评价体系。该体系不仅关注模型是否成功移除了有害概念，同样重视擦除操作对视频生成质量与时序连贯性的潜在影响。

在安全性评估维度，实验采用 CLIPScore 作为核心指标以量化概念擦除的有效性。该指标基于 CLIP 视觉-语言预训练模型~\cite{Radford2021LearningTV}，旨在度量生成内容与目标概念在语义空间中的重叠程度。形式化地，给定生成的视频序列 $V = \{v_1, v_2, \dots, v_T\}$ 以及描述目标概念的提示词文本 $c$，单个视频帧 $v_t$ 的语义相关度得分 $S_{\text{CLIP}}(v_t, c)$ 被定义为图像嵌入向量 $E_I(v_t)$ 与文本嵌入向量 $E_T(c)$ 之间的余弦相似度：

\begin{equation}
    S_{\text{CLIP}}(v_t, c) = \frac{E_I(v_t) \cdot E_T(c)}{|E_I(v_t)|2 |E_T(c)|2} \times 100.
\end{equation}

得分越低，意味着生成的视频内容与目标概念的语义距离越远，即擦除效果越显著。鉴于简单的平均相似度可能掩盖偶发的擦除失效情况，实验在汇报全序列平均得分 $S{\text{avg}} = \frac{1}{T}\sum{t=1}^{T} S_{\text{CLIP}}(v_t, c)$ 之外，额外引入了前百分之一的极端高分指标 $S_{\text{top1\%}}$。该指标通过计算所有测试样本帧级得分分布的第 99 百分位数值获得。数学上，它能够敏锐地捕捉到视频序列中残留的最强概念特征，实质上衡量了最坏情况下的隐私泄漏或版权侵权风险，从而为安全评估提供了更为严苛的底线保障。

针对生成视频的实用性与画质评估，本研究首先引入 DOVER 评分模型~\cite{wu2023dover}。该模型作为一种无参考视频质量评估方法，采用了双流网络架构从美学观感与技术质量两个互补视角对视频进行打分。其中美学评分侧重于通过自适应的主观感知模块评估构图、光影与色彩表达的艺术性，而技术评分则专注于量化分辨率、噪点水平以及画面的清晰度。这一双重视角确保了模型在移除特定概念后，并未因参数微调而导致整体视觉体验的退化。

此外，为了深入剖析视频内容的结构完整性与时序连贯性，实验采用了 VBench-2 评测套件~\cite{huang2023vbench,zheng2025vbench2,huang2025vbench++}。该套件提供了细粒度的维度分析，包括人体解剖结构合理性与人物特征一致性指标。前者用于检测生成主体是否存在肢体畸形，后者则通过计算生成人物面部特征与参考特征之间的相似度矩阵来量化身份特征的稳定性。更为关键的是，实验重点考察了多视角一致性指标，这在单目视频生成的语境下被具体化为时序一致性。该指标 $S_{\text{temp}}$ 被定义为视频序列中相邻帧之间视觉特征的余弦相似度均值：\begin{equation}S_{\text{temp}}(V) = \frac{1}{T-1} \sum_{t=1}^{T-1} \text{sim}\left( \mathcal{F}(v_t), \mathcal{F}(v_{t+1}) \right),\end{equation}其中 $\mathcal{F}(\cdot)$ 表示特征提取函数。若该指标得分显著下降，通常意味着视频出现了画面闪烁、非物理的形变或运动轨迹的突变。因此，该指标的数值变化趋势直接验证了时空-语义对齐损失在维持视频时序结构方面的有效性，是衡量算法是否成功锁定时空骨架的关键依据。

\subsection{核心性能定量分析}

\begin{table*}[t]
    \centering
    \caption{不同方法在 VCE-24K 测试集上的综合性能评估。本表涵盖安全性、视觉美学、视频质量及文本一致性四个维度。其中 $\downarrow$ 表示数值越低越好，$\uparrow$ 表示数值越高越好。加粗数据表示在同类比较中的最优结果（不包含参考组）。}
    \label{tab:main_results}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccccccccc}
    \toprule
    \multirow{3}{*}{\textbf{方法 (Method)}} & \multicolumn{2}{c}{\textbf{安全性}} & \multicolumn{2}{c}{\textbf{美学评分 (DOVER)}} & \multicolumn{4}{c}{\textbf{VBench-2 综合视频质量}} & \multicolumn{1}{c}{\textbf{控制力}} \\
    & \multicolumn{2}{c}{\textit{(Safety)}} & \multicolumn{2}{c}{\textit{(Aesthetics)}} & \multicolumn{4}{c}{\textit{(Video Quality \& Consistency)}} & \multicolumn{1}{c}{\textit{(Alignment)}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-9} \cmidrule(lr){10-10}
    & \textbf{CLIP-Avg} $\downarrow$ & \textbf{Top-1\%} $\downarrow$ & \textbf{美学分} $\uparrow$ & \textbf{技术分} $\uparrow$ & \textbf{成像质量} $\uparrow$ & \textbf{解剖结构} $\uparrow$ & \textbf{运动平滑度} $\uparrow$ & \textbf{时序一致性} $\uparrow$ & \textbf{文本对齐} $\uparrow$ \\
    \midrule
    \multicolumn{10}{l}{\textit{Reference Group (参考基准)}} \\
    \textbf{未擦除} (Original) & 26.15 & 33.06 & 10.21 & \textbf{10.15} & \textbf{82.50} & \textbf{99.68} & \textbf{98.20} & \textbf{22.11} & \textbf{0.320} \\
    \midrule
    \multicolumn{10}{l}{\textit{Inference-time Intervention (推理干预)}} \\
    \textbf{免训练} (No Training) & 18.51 & 25.65 & 10.18 & 9.85 & 81.20 & 99.13 & 94.50 & 16.42 & 0.305 \\
    \midrule
    \multicolumn{10}{l}{\textit{Model-level Editing (模型编辑 - Ours)}} \\
    \textbf{无 STSA} (w/o STSA) & 19.31 & 26.05 & 10.08 & 9.92 & 80.45 & 99.12 & 85.40 & 11.84 & 0.310 \\
    \textbf{无 DPO} (w/o DPO) & 20.80 & 27.40 & 10.12 & 10.05 & 81.80 & 99.24 & 97.10 & 20.42 & 0.315 \\
    \textbf{全量方法} (Full Method) & \textbf{18.40} & \textbf{25.52} & \textbf{10.20} & 10.12 & 82.10 & 99.41 & 97.80 & 19.80 & 0.318 \\
    \bottomrule
    \end{tabular}
    }
    \end{table*}

表~\ref*{tab:main_results}详细展示了不同实验配置下模型在安全性、美学质量及视频结构完整性三个维度的量化评估结果。数据表明，本文提出的全量方法（Full Method）在有效移除高风险概念的同时，成功维持了接近原始模型的生成画质与时空一致性，验证了潜空间偏好优化策略的优越性。

首先关注安全性维度的表现。与未经过任何处理的原始模型（No Erasure）相比，全量微调后的模型在 CLIPScore 平均得分上实现了显著下降，从 26.15 降低至 18.40。这一降幅表明生成内容与目标概念之间的语义关联被大幅削弱，模型已不再倾向于响应包含敏感特征的生成请求。更为关键的是，在前百分之一的极端高分指标上，全量方法将得分从 33.06 压制至 25.52。极端分数的显著回落具有重要的物理意义，它意味着模型在面对最具诱导性的对抗性提示词或随机种子时，依然能够守住安全底线，有效地遏制了“最坏情况”下的隐私泄漏或版权侵权风险。

在确保安全性的前提下，全量方法在画质保持方面展现出了极高的稳定性。观察 DOVER 美学评分与 VBench 解剖结构评分（Anatomy），全量方法相较于原始模型的得分降幅极微，分别仅下降了 0.01 与 0.27 分。这种近乎无损的性能表现有力地证明了该方法并未引发灾难性遗忘。不同于全参数微调可能导致的模型能力坍塌，基于低秩自适应的局部更新策略如同精准的“神经外科手术”，成功地将修改范围限制在了特定的概念表征空间内，从而完好地保留了预训练模型对光影、纹理及人体结构的通用建模能力。

最后，通过对比免训练策略（No Training）与全量方法的性能差异，揭示了参数化微调的必要性。虽然免训练方法在 CLIPScore 上也取得了具有竞争力的低分（18.51），但其在多视角一致性指标（Consistency）上表现欠佳，仅获得 16.42 分，显著低于全量方法的 19.80 分。这一巨大的分差反映了单纯依靠推理阶段的路径分离技巧难以维持长视频序列的稳定性，极易导致画面随时间推移而崩解。相反，全量方法通过在训练过程中引入时空-语义对齐约束，将时序连贯性内化为模型参数的一部分。这使得生成的视频不仅在单帧上符合安全规范，在整个时间轴上也保持了流畅自然的运动逻辑，证明了针对视频任务进行专门的参数化适配是不可或缺的。

\subsection{消融实验与机制验证}

\begin{table}[t]
    \centering
    \caption{LoRA 低秩矩阵维度（Rank）对擦除效能与生成质量的敏感性分析。数据表明 Rank=64 是平衡安全性与画质的最佳选择。}
    \label{tab:ablation_rank}
    \resizebox{0.85\linewidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{\textbf{LoRA Rank}} & \multicolumn{2}{c}{\textbf{安全性 (Safety)}} & \multicolumn{2}{c}{\textbf{画质与一致性 (Quality)}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    & \textbf{CLIP-Avg} $\downarrow$ & \textbf{Top-1\%} $\downarrow$ & \textbf{DOVER 综合分} $\uparrow$ & \textbf{时序一致性} $\uparrow$ \\
    \midrule
    \textbf{Rank = 32} (欠拟合) & 19.95 & 27.10 & \textbf{74.20} & \textbf{21.05} \\
    \textbf{Rank = 64} (本文配置) & \textbf{18.40} & 25.52 & 73.98 & 19.80 \\
    \textbf{Rank = 128} (过拟合) & 18.12 & \textbf{24.95} & 71.50 & 14.30 \\
    \bottomrule
    \end{tabular}
    }
    \end{table}

为了深入剖析本研究提出的框架中各核心模块的独立贡献，实验设计了一系列严谨的消融测试。通过选择性地移除时空-语义对齐损失或潜空间偏好优化目标，观测了各组件对最终安全性与生成质量的具体影响机制。

首先考察时空-语义对齐损失在维持视频动态连贯性方面的关键作用。当从训练目标中移除该对齐约束时，模型在语义相关度指标上依然维持了较低的水平。这表明仅依靠偏好优化已足以驱动模型在语义层面剥离目标概念。然而，多视角一致性得分却出现了断崖式的下跌。视觉层面的深入分析揭示，在缺乏对齐约束的情况下，生成的视频帧出现了严重的帧间闪烁与非物理性的背景突变。这一现象有力地证明了时空-语义对齐机制在视频擦除任务中扮演着稳定器的角色。它通过强制约束注意力上下文，确保模型在修改局部语义的同时锁定了整体的时空骨架，从而避免了视频结构的崩塌。

反之，实验验证了潜空间偏好优化目标的核心地位。当移除偏好优化损失而仅保留对齐约束时，语义相关度得分出现了大幅回升。这意味仅靠对齐约束无法提供足够的负向梯度来推离目标概念的分布。在定性分析中，生成的视频中仍残留着明显的视觉特征，例如特定角色的标志性服饰配色或面部轮廓依然清晰可辨。这一结果确认了偏好优化损失提供了必要的优化动力，是实现概念剥离的根本驱动引擎。

此外，表~\ref{tab:ablation_rank} 展示了低秩自适应矩阵的秩对模型综合性能的敏感性影响，揭示了参数容量与擦除任务之间的非线性权衡关系。实验结果呈现出清晰的倒 U 型效能曲线。当秩设定为 32 时，由于可训练参数量过少，模型难以拟合高维潜空间中复杂的概念擦除边界。这表现为欠拟合状态，此时虽然视频画质与时序一致性极高，但安全性指标 CLIP-Avg 仅降至 19.95，说明目标概念仍有显著残留。相反，将秩增加至 128 虽然能进一步将安全性指标压低至 18.12，但也引入了显著的过拟合风险。过量的参数更新破坏了预训练模型的先验分布，导致 DOVER 综合分与时序一致性大幅下滑，视频出现明显的画质退化与结构崩塌。因此，秩为 64 的配置在保证充分擦除效能的同时，最大限度地维持了生成质量，被证实为最佳的平衡点。


\subsection{用户主观评测}
\begin{table}[t]
\centering
\caption{双盲用户主观评测结果。HCR (Human Concept Recall) 衡量人类对目标概念的识别率，HLR@4 (High-Confidence Leakage) 衡量高置信度泄漏，MOS 为视频整体质量平均意见分（1-5分）。}
\label{tab:user_study}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{方法 (Method)} & \textbf{人类检测率 (HCR)} $\downarrow$ & \textbf{高置信泄漏 (HLR@4)} $\downarrow$ & \textbf{质量评分 (MOS)} $\uparrow$ \\
\midrule
\textbf{未擦除} (No Erasure) & 98.2\% & 94.5\% & \textbf{4.36} \\
\textbf{免训练} (No Training) & 21.5\% & 14.2\% & 4.05 \\
\midrule
\textbf{无 STSA} (w/o STSA) & 26.8\% & 19.5\% & 3.80 \\
\textbf{无 DPO} (w/o DPO) & 37.4\% & 29.1\% & 3.32 \\
\textbf{全量方法} (Full Method) & \textbf{18.1\%} & \textbf{12.3\%} & \textbf{4.18} \\
\bottomrule
\end{tabular}
}
\end{table}

尽管前文所述的自动化指标在量化模型性能方面提供了客观依据，但其在捕捉人类细腻的视觉感知方面仍存在固有的局限性。例如，CLIPScore 虽然能衡量语义相似度，却难以识别出某些虽去除了显式特征、但仍保留了角色“神韵”或特定行为模式的隐性泄漏。为了弥补单一技术指标的不足，确证算法在真实应用场景中的鲁棒性，本研究进一步开展了一项严格的双盲用户主观评测。该评测旨在引入人类视觉认知作为终极判别标准，对生成视频的安全性边界与画质体验进行综合考量。

在实验设计上，为了确保评估结果的统计学代表性与公正性，我们邀请了五十名具有不同年龄、性别及专业背景的评估员参与测试。这些参与者既包含具备一定视觉设计背景的专业人士，也涵盖了普通的视频内容消费者，从而全面模拟了真实的用户群体构成。评测采用了严格的双盲机制（Double-blind），即在打分过程中，所有视频样本均隐去了生成方法的标签，且播放顺序经过了完全的随机化处理。这一机制有效地消除了评估员因先验知识或顺序效应而产生的心理偏差，确保了每一项评分均纯粹基于视觉内容本身。

在样本构建方面，实验从测试集中精选了一百二十个高风险提示词。这些提示词不仅覆盖了静态的人物肖像，还包含了复杂的动态交互与多样的场景转换，这对视频生成的一致性提出了极高挑战。针对每一个提示词，我们分别利用原始模型、免训练基线、两个消融变体以及本文提出的全量方法生成了对应的视频片段，累计构建了包含六百个独立样本的评测数据集。评估员被要求在观看完每个视频后，从“概念是否可辨识”的二元安全性维度，以及“视频流畅度与美观度”的李克特量表（Likert Scale）质量维度进行独立打分。这种大规模、多维度的细粒度人工评估，为验证算法的实际效能提供了最为坚实的主观数据支撑。


评估维度涵盖安全性与生成质量两个核心方面。安全性评估主要通过人类概念召回率（HCR）来衡量，即统计观察者能否从视频中识别出目标概念的比例。同时，实验记录了高置信度泄漏率（HLR），用于捕捉观察者非常确定目标概念依然存在的极端情况。生成质量则采用平均意见分（MOS）进行量化，综合考察视频的流畅度、美观度以及是否存在明显的编辑伪影。

表~\ref{tab:user_study} 汇总了五十名评估员在双盲条件下对六百个视频样本的评测结果。数据表明，本文提出的全量方法在安全性与实用性的人类感知层面均取得了显著成效。在安全性维度，与原始模型高达 98.2\% 的人类检测率相比，全量方法将这一数值骤降至 18.1\%。更为关键的是，衡量严重隐私泄漏的高置信度泄漏率（HLR@4）仅为 12.3\%。这一数据具有重要的现实意义，它表明在绝大多数视觉场景下，普通人类观察者已无法从生成的视频中识别出原始的版权角色或敏感特征，证明了算法在切断视觉联想方面的有效性。

在生成质量的主观感知方面，全量方法获得了 4.18 的平均意见分（MOS）。这一数值非常接近原始模型的 4.36 分，且显著高于移除时空-语义对齐损失的变体（3.80 分）。评估员的反馈普遍表明，全量方法生成的视频动作自然流畅，未出现明显的画面闪烁或结构崩塌。相比之下，无 STSA 的变体因帧间不连贯而导致评分较低，而无 DPO 的变体则因概念擦除不彻底导致内容混淆，评分最低。主观评测的结果与前文所述的自动化指标呈现出高度的一致性。这种跨评价维度的一致性不仅验证了实验结论的鲁棒性，也进一步确认了该方案在实际工业部署中具备极高的人类可接受度与实用潜力。


\section{本章小结}\label{sec:summary4}

本章针对视频生成模型内生安全机制构建中面临的计算资源瓶颈与时空一致性维持难题，系统地提出了一种基于潜空间偏好优化的视频概念擦除方法。通过深入剖析扩散模型的去噪轨迹与生成概率之间的理论联系，本研究成功将复杂的强化学习目标转化为无需显式奖励模型的监督优化问题。这一范式转换使得在低维潜空间内直接进行对抗性微调成为可能，从而有效规避了传统像素级监督方法所带来的高昂解码开销，为在有限算力下实现大模型的安全对齐奠定了基础。

在此基础上，为了解决视频生成特有的帧间闪烁与结构崩塌问题，本章构建了时空-语义对齐损失机制。该机制独辟蹊径地利用注意力模块中的键值上下文作为时空骨架的锚点，在驱动模型参数剥离目标语义特征的同时，强制约束了视频的物理布局与运动逻辑。这种设计巧妙地实现了语义内容与时空结构的有效解耦，确保了擦除操作如同精准的外科手术，仅移除有害概念而未对通用的生成能力造成灾难性遗忘。

基于 VCE-24K 基准数据集的广泛实验评估有力地支撑了上述理论假设。定量分析数据显示，所提方法在显著降低高风险概念语义相关度的同时，近乎无损地保留了原始模型的生成画质与时序连贯性。多维度的消融实验深刻揭示了对齐约束在维护视频动态稳定性中的决定性作用，而低秩矩阵容量的敏感性分析则为参数配置提供了经验指导。双盲用户主观评测进一步确认了该技术方案在人类感知层面的优越性与实用性。综上所述，本章的研究成果证实了在模型参数层面进行精准概念修正的可行性，为构建安全可控的视频生成系统提供了坚实的理论支撑与高效的技术路径。