\chapter{绪论}\label{chap:introduction}

\section{研究背景与意义}

\subsection{生成式人工智能与文生视频技术的爆发}

生成式人工智能（Artificial Intelligence Generated Content, AIGC）是继专业生产内容、用户生产内容后新型内容创作方式。
近年来，生成式人工智能经历了从静态图像向动态视频跨越的爆发式增长。
早期的视频生成方法多依赖于生成对抗网络（GANs）~\citep{goodfellow2014generative}，受限于训练不稳定和模式崩塌等问题，往往难以生成高分辨率且时序连贯的长视频。
随着扩散模型（Diffusion Models）~\citep{Ho2020DenoisingDP, Song2020ScoreBasedGM}与多模态对齐技术的迅速演进，计算机视觉领域的研究重心正从传统的判别式任务向生成式与理解-生成统一任务发生深刻转变。
扩散模型早期由于计算成本高昂，无法有效降低采样效率，只能用于图像生成。
随着对扩散模型的深入研究，扩散模型的架构、采样空间、推理过程、控制方法等得到了显著优化，为文生视频技术的快速发展奠定了坚实基础。
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{Img/figure_intro_aigc.png}
    \bicaption{生成式人工智能内容创作示意图}{Illustration of Generative AI content creation}
    \label{fig:aigc}
\end{figure}

随着以 Diffusion Transformer (DiT) ~\citep{dit}为代表的新一代架构的提出，文本生成视频（Text-to-Video, T2V）技术取得了突破性进展。
该类模型成功将 Transformer~\citep{vaswani2017attention}的可扩展性与扩散模型的高质量、生成多样性相结合，能够根据简短的文本描述生成符合物理规律、具备高度时间连贯性且具备一定美学的视频内容。
此外，由于 Transformer 架构在其它领域的广泛应用，针对 Transformer 架构的优化技术也得到了显著发展，为文生视频技术的计算效率与内存消耗提供了有力支持。
因此文生视频能在海量数据下进行高效训练，生成逼真且时序连贯的视频内容。

其中，OpenAI 发布的 Sora 模型~\citep{sora}标志着该领域的里程碑式飞跃，其展示了在复杂场景构建、物理世界模拟及长视频生成方面的惊人能力。此外，国内平台如快手、阿里、腾讯等亦涌现出以可灵～\citep{Kling}、万象~\citep{wan2.1}和混元~\citep{hunyuanvideo}为代表的先进模型。
例如，Wan-2.1 等模型已能够在消费级或科研级算力下，生成分辨率达 832×480、帧率 16fps 且结构连贯的视频片段。
这些技术突破使得视频生成不再局限于实验室研究，而是展现出强大的应用潜力，被广泛应用于自动内容创作、个性化动画制作、影视预演及虚拟现实等领域。

文生视频技术的爆发不仅极大地降低了视频创作的门槛，赋予了创作者前所未有的·生成自由度，同时也使得生成内容在视觉上更加逼真、在语义上更加丰富。
这种技术范式的升级，为多媒体内容生产带来了革命性的机遇，但随之而来的内容合规性与安全性问题也变得愈发严峻。

\subsection{安全风险与伦理挑战}

随着生成式人工智能模型在参数规模与生成质量上的飞跃，其带来的负面外部性也日益凸显。
高质量的视频生成能力意味着模型对物理世界与人类社会的模仿更加逼真，这种“拟真性”在赋能创作的同时，也引发了前所未有的安全风险与伦理挑战。
当前，文生视频技术面临的威胁主要集中在版权侵权、身份滥用以及有害内容生成三个维度。

首先，版权侵权（Copyright Infringement）已成为制约该技术商业化落地的最大法律阻碍。 
深度生成模型在预训练阶段通常需要吞噬海量的互联网数据，这一过程往往会导致模型产生“记忆化”（Memorization）现象，即模型在生成阶段无意间复现训练集中的特定样本。
当这种复现涉及受版权保护的知名IP时，便构成了直接侵权。 
例如，在2025年9月迪士尼企业及其他11家制片公司对AI公司MiniMax提起的诉讼中，原告指出，用户只需输入简单的文字指令，模型便能生成包含达斯·维达（Darth Vader）、小黄人（Minions）、超人等知名角色的高清图像与视频。
类似的争议在国内也已出现，如“奥特曼AI生成侵权案”中，法院判决认定平台若未采取有效过滤措施，导致生成的图片与奥特曼形象高度相似，需承担相应的侵权责任。
这些案例表明，现有的生成模型极易成为“洗稿”或“盗版”的工具，对创意产业的知识产权构成了系统性威胁。

其次，名人身份滥用（Identity Misuse）与深度伪造（Deepfake）引发了严重的社会信任危机。 
得益于时空一致性的提升，文生视频模型能够生成连贯的、难辨真伪的人物动态影像。
这种能力一旦被恶意利用，即可针对现实中的政治人物、演艺明星或普通公众制造虚假视频。 
例如，通过特定提示词生成政治人物发表不当言论的视频，可能被用于制造假新闻，干扰舆论甚至破坏选举秩序；而针对演艺明星生成的虚假商业代言或不雅视频，则严重侵犯了肖像权与名誉权。
与静态的“换脸”图像相比，视频模态的深度伪造包含动作与表情的动态变化，其欺骗性更强，传播危害性更大，给公众的信息甄别带来了极大挑战。

最后，有害内容生成（Harmful Content Generation）是内容安全领域面临的长期顽疾。 
尽管现有的模型服务商通常会部署安全过滤机制，但在复杂的对抗攻击面前往往显得脆弱。
恶意用户可以通过精心设计的“越狱提示词”（Jailbreak Prompts）——即通过隐晦、隐喻或角色扮演等方式绕过文本审查，诱导模型生成暴力、血腥、色情或宣扬仇恨的违规视频。 
部分AI公司虽然具备识别暴力或裸露内容的技术能力，但在利益驱动下，往往选择不将此类保护措施严格应用于所有受版权保护或敏感的内容。
这种技术能力与防御意愿之间的不对等，使得文生视频模型可能沦为有害信息传播的温床，对网络生态安全构成威胁。

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{Img/figure_intro_risks.png}
    \bicaption{文生视频模型面临的安全风险示例}{Examples of security risks faced by text-to-video models}
    \label{fig:risks}
\end{figure}

\subsection{研究意义}

在生成式人工智能技术加速落地的背景下，构建面向文生视频模型的全流程安全防御体系，对于保障 AIGC 产业的健康发展与维护社会公共安全具有重要的理论意义与实用价值。

\textbf{（1）应对复杂对抗攻击的必然要求} 

当前，针对生成式模型的攻击手段正变得愈发隐蔽和多样化。传统的单一防御手段已显露出明显的局限性：仅依靠输入端的关键词黑名单或简单的分类器过滤，极易被恶意的“越狱提示词”（Jailbreak Prompts）通过隐喻、角色扮演或多语言混淆等方式绕过 ；而仅依靠模型端的后处理拦截，则无法从根本上消除模型内部存储的风险知识，且在面对高并发请求时往往带来巨大的计算延迟。因此，建立一套涵盖“输入端语义理解前置拦截”与“模型端风险概念内生擦除”的全流程防御体系，形成“事前预防”与“源头治理”互为补充的纵深防御机制，是应对复杂安全威胁的必然选择。

\textbf{（2）填补视频生成安全系统化研究的空白} 

现有关于生成式内容安全的研究多聚焦于图像模态，或割裂地关注检测与擦除的单一环节。针对视频生成任务，由于其独特的时空高维特性，直接迁移图像领域的防御算法往往面临计算成本过高或破坏时序连贯性的难题 。本研究提出的基于大语言模型思维链的检测方法与基于潜空间偏好优化的擦除算法，不仅解决了视频模态下的效率与一致性瓶颈，更尝试将二者集成于统一的系统架构中 。这为构建高效、可控的视频生成安全系统提供了新的理论视角与技术路径，有助于完善生成式人工智能安全治理的技术拼图。

\textbf{（3）护航 AIGC 产业合规与知识产权保护} 

随着《生成式人工智能服务管理暂行办法》等法规的出台，内容合规已成为 AIGC 企业的生命线。本研究构建的防御体系，一方面能够通过高效的概念擦除技术，帮助模型服务商在不重新训练模型的前提下，低成本地移除受版权保护的角色（如知名动画形象）或特定人物身份，规避知识产权侵权与肖像权纠纷 ；另一方面，前置安全检测模块能够有效拦截暴力、色情等有害内容的生成请求，降低平台运营的合规风险。这对于推动文生视频技术从实验室走向大规模商业应用，构建可信、负责任的人工智能生态具有显著的现实意义。

\section{国内外研究现状}\label{sec:system}

\subsection{文本生成视频模型的发展现状}

近年来，神经视频生成领域经历了深刻的架构范式转移。
随着深度学习技术的演进，视频生成已从早期的生成对抗网络（GAN）彻底转向了基于扩散模型（Diffusion Models）的生成范式，并在近期迎来了从卷积神经网络（CNN）向 Transformer 架构的决定性跨越。

\subsubsection{从对抗生成到扩散模型的范式确立} 

在深度学习应用于视频生成的初期，技术路线主要由生成对抗网络（GAN）~\cite{goodfellow2014generative}和递归神经网络（RNN）~\cite{rumelhart1986learning,goodfellow2016deep}主导。
然而，由于视频数据的高维特性，GAN 在长视频生成中面临着严重的“模式坍塌”（Mode Collapse）问题，即样本多样性急剧下降。
此外，仅凭对抗损失难以约束长距离的时间连贯性，常导致伪影与逻辑跳变。

而 RNN 的递归结构的串行计算特性限制了并行训练能力与数据吞吐量。长短期记忆网络（LSTM）在处理长序列时依然面临梯度消失与长程依赖（Long-range Dependency）捕捉困难的问题，导致生成视频随时间推移迅速模糊。~\cite{hochreiter1997long}
相比之下，扩散模型~\cite{Song2020ScoreBasedGM, Ho2020DenoisingDP}通过将生成过程建模为逆向马尔可夫链，逐步去除噪声以恢复数据分布，具备更稳定的训练收敛性与更强的分布覆盖能力，从而确立了其在视频生成领域的主流地位。

\subsubsection{架构演进：从 3D U-Net 到 Diffusion Transformer (DiT)} 

初期视频扩散模型（如 AnimateDiff~\cite{guo2023animatediff}、Stable Video Diffusion~\cite{blattmann2023stable} 以及 VDM~\cite{ho2022video}）多采用“图像到视频”（Image-to-Video Adaptation）的迁移策略，即基于 2D U-Net 进行时空膨胀。
通过将 2D 卷积核扩展为伪 3D 卷积（$(1 \times k \times k)$）~\cite{singer2022make} 或插入时间注意力模块（Motion Modules）来适配视频数据。
但是，U-Net 依赖多尺度下采样/上采样，这种刚性的压缩结构限制了对不同帧率与时长的适应性。且卷积操作的局部感受野（Local Receptive Field）限制了长程时空依赖的捕捉，导致长视频首尾帧逻辑断裂。

相比之下，DiT 架构~\cite{peebles2023scalable} 摒弃了 U-Net 的归纳偏置，将视频数据序列化为 Token 流。以 Latte~\cite{ma2024latte} 和 Sora~\cite{brooks2024video} 为代表的方法引入了“时空补丁”（Spacetime Patches）机制，通过变分自编码器（VAE）将视频压缩至隐空间，并将 3D 隐体积切分为时空补丁。
Transformer 的全局自注意力机制直接对时空依赖进行建模，允许任意时空位置的 Token 进行直接交互，彻底解决了长程依赖问题。
DiT 架构的生成质量随计算量（FLOPs）增加呈现出可预测的线性提升，符合缩放定律（Scaling Laws）~\cite{kaplan2020scaling, peebles2023scalable}，也是 Sora 等模型能够涌现出对物理世界的模拟能力、自发习得 3D 场景的内部表征以保持透视与几何一致性的关键~\cite{brooks2024video}。

\subsubsection{前沿模型技术特性分析}
当前最具代表性的开源视频生成模型在 DiT 架构基础上进行了进一步的优化与创新。
首先，在长序列建模的高效性方面，针对视频生成固有的高计算复杂度挑战，以 HunyuanVideo~\cite{hunyuanvideo} 为代表的模型提出了选择性滑动块注意力（SSTA）机制。该机制通过稀疏化与动态剪枝策略，在保持时空平滑性的同时大幅降低了显存开销，有效克服了传统全注意力机制 $O(N^2)$ 的复杂度瓶颈。

其次，流匹配与因果时空压缩技术成为提升生成质量的关键。以本文实验所使用的 \textbf{Wan-2.1} 模型~\cite{wan2.1} 为例，其核心技术路线融合了流匹配（Flow Matching）~\cite{lipman2022flow} 与 3D 因果变分自编码器（3D Causal VAE）~\cite{blattmann2023stable}。其中，流匹配技术通过构建平直的常微分方程（ODE）生成轨迹，显著减少了采样步数，实现了高质量视频的快速生成；而 3D 因果 VAE 则通过强制当前帧的压缩仅依赖于历史信息，从根本上消除了传统 3D 卷积可能导致的“未来信息泄露”问题。结合特征缓存机制，该架构使得模型在消费级显卡上亦能支持长视频的流式推理。

综上所述，当前的文生视频技术已具备极高的生成质量与时空连贯性。然而，正是这种深度的时空纠缠特性，使得传统的基于图像的防御手段难以直接迁移，为视频内容的安全治理与概念擦除带来了全新的挑战。

\subsection{生成式内容安全防御研究现状}

随着生成式人工智能从静态图像向动态视频领域的范式转移，基于扩散变换器（Diffusion Transformer, DiT）的视频生成模型（如 Sora、HunyuanVideo、Wan 2.1 等）在解决时序一致性与高分辨率渲染方面取得了突破性进展。然而，这一跨越同时也重塑了内容安全的威胁面。视频内容不仅继承了图像生成的安全风险，更引入了时序隐蔽性与多模态攻击向量等特有挑战。传统的基于关键词匹配或静态图像分类的防御机制已难以应对离散提示优化攻击与隐性隐喻攻击。因此，学术界与工业界逐渐确立了“输入端深层语义检测”与“模型端高效概念擦除”并行的双重防御体系。

\subsubsection{输入端防御：从关键词过滤到多模态语义推理}

输入端防御作为安全的第一道防线，其研究重点已从浅层的规则匹配转向基于大语言模型（LLM）的深层意图识别与推理。

针对简单的显式违规，Meta 提出的 Llama Guard 3 Vision~\cite{chi2024llama}代表了基于指令微调的通用防御思路。该类方法通过建立涵盖暴力、色情、非暴力犯罪等维度的详尽危害分类体系（Taxonomy），利用多模态大模型对用户输入的文本与图像提示进行联合分析与拦截。然而，面对利用社会工程学或认知偏差设计的“越狱”攻击，单纯的分类模型往往显得力不从心。

为了提升防御的鲁棒性，研究者开始引入强化学习与推理机制。以 VideoSafety-R1 框架~\cite{sun2025evaluation}为代表的研究提出了“安全即推理”的理念。该框架通过引入警报令牌（Alarm Tokens）作为认知锚点，并结合群组相对策略优化（GRPO）算法~\cite{grpo}，使模型能够主动推演输入指令在视频生成后的潜在社会影响，从而有效识别包含隐性隐喻或对抗性扰动的复杂攻击。此外，WildGuard~\cite{han2024wildguard} 等工具通过在输入端拦截与输出端审核的双向调节机制，进一步降低了漏判率。

\subsubsection{模型端防御：高效概念擦除与机器遗忘}

鉴于大型视频生成模型全量重新训练（Retraining）的高昂算力与时间成本，如何在不破坏模型通用生成能力的前提下，精准、高效地移除有害知识（Concept Erasure/Unlearning），成为模型端防御的核心议题。当前技术主要分为优化基方法、闭式解方法与推理干预方法三类。

\textbf{（1）基于约束优化的擦除技术。} ConceptVoid 等方法~\cite{huang2025conceptvoid}将概念擦除建模为约束多目标优化问题。通过最大化目标概念的生成难度，同时利用 KL 散度约束模型在非目标概念上的行为差异，以防止灾难性遗忘。针对多概念擦除中的梯度冲突问题，该类方法引入多梯度下降算法（MGDA）寻找帕累托最优解，实现了多重有害概念的同步剥离。

\textbf{（2）基于闭式解的快速编辑。} 为了追求极致的编辑速度，ICE（Instant Concept Erasure）等研究~\cite{biswas2025now}提出了基于谱分析的闭式解方案。该方法利用子空间正交化原理，计算出解离算子，将模型权重矩阵投影至目标概念向量的零空间（Null Space）。这种方法避免了迭代训练，实现了毫秒级的权重编辑，且因其基于全局线性变换，在视频生成任务中表现出优异的时空一致性。

\textbf{（3）无需训练的推理干预。} 针对无法访问或修改模型权重的场景，SAFREE 等框架~\cite{yoon2024safree}提出了推理阶段的动态防御机制。通过监控扩散模型去噪过程中的自注意力图（Self-Attention Maps）与文本嵌入空间，实时检测并压制潜在的违规特征激活。这种即插即用的方法具有高度的自适应性，能在不影响模型主体架构的情况下实现防御。

\subsubsection{工业界实践与评估基准}

在工业界，HunyuanVideo~\cite{hunyuan-video} 与 Sora~\cite{sora} 等前沿模型已开始在架构层面融入安全设计。例如，采用双流（Dual-stream）架构实现文本与视觉特征的早期解耦审查，利用稀疏注意力机制（SSTA）~\cite{hunyuanvideo2025}进行关键区域的快速扫描，并集成 C2PA 数字签名与 SynthID 隐形水印技术以增强内容的可溯源性。

在评估体系方面，T2VSafetyBench~\cite{miao2024t2vsafetybench}等基准测试集的建立，将视频安全评估维度扩展至时序动作风险与版权侵犯等特定领域，结合攻击成功率（ASR）与语义距离（MM-Notox Distance）等量化指标，为验证防御技术的有效性提供了科学依据。

综上所述，当前生成式视频内容安全研究正处于从单点防御向体系化、智能化防御演进的关键阶段。结合输入端的深层语义推理与模型端的精确概念擦除，是应对日益复杂的生成式AI安全威胁的必然选择。
\subsection{现有防御手段的局限性分析}

\section{本文主要研究内容}

针对现有文生视频安全防御中存在的输入端隐晦攻击难识别、模型端擦除计算成本高且易破坏时序连贯性，以及缺乏系统化防御架构等问题，本文提出了一套面向文生视频模型的全流程安全防御体系。主要研究内容包括以下三个方面：

\subsection{基于思维链增强的视频生成前置安全检测}

针对输入端恶意提示词具有隐蔽性、多义性及“越狱”攻击频发的特点，本文提出了一种基于大语言模型（LLM）思维链增强的前置检测框架。

（1）模型构建与微调： 选用轻量化的 Llama-3.1-8B ~\cite{dubey2024llama}作为基座模型，采用参数高效微调（PEFT）技术注入特定领域的安全知识，使其能够适配视频生成的上下文语境。

（2）推理增强策略： 设计了“策略即提示（Policy-as-Prompt）”机制，将复杂的安全审核规则转化为显式的系统提示；同时引入“思维链（Chain-of-Thought, CoT）”推理范式~\cite{wei2022chain}，引导模型在输出最终判定前生成逐步的逻辑推理轨迹。这一机制显著提升了模型对隐喻、角色扮演及多语言混淆等“灰域”语义的感知能力，实现了对高风险生成请求的精准拦截。

\subsection{基于潜空间直接偏好优化的视频概念擦除}

针对现有图像擦除方法直接迁移至视频领域时面临的计算开销过大及视频时空结构崩塌问题，本文提出了一种无需解码的高效视频概念擦除方法 。

（1）Latent-DPO 算法： 提出潜空间直接偏好优化（Latent-DPO）算法，摆脱了对视频逐帧像素级解码的依赖。通过在统一噪声输入与时间步下构建“概念/无概念”正负潜变量轨迹，直接在扩散模型的潜空间内对去噪路径进行偏好对齐，大幅降低了显存占用与训练时间。

（2）时空-语义对齐损失（STSA）： 为解决擦除过程中可能引发的视频闪烁与非目标语义漂移，设计了基于交叉注意力机制的时空-语义对齐损失（STSA）。该损失函数通过约束查询（Query）与键（Key）的交互上下文，确保模型在移除目标概念的同时，保持视频原本的时序连贯性与结构完整性。

（3）VCE-24K 基准数据集构建： 针对视频擦除领域数据匮乏的现状，利用冻结的 Wan-2.1 模型与自动化筛选流水线，构建了首个包含 24,000 对样本的视频概念偏好数据集 VCE-24K，填补了该领域的数据空白。

\subsection{文生视频全流程安全防御系统设计与实现}

为了验证上述算法的有效性并推动技术落地，本文集成前置检测模块与内部擦除算法，设计并实现了一套文生视频全流程安全防御系统。

（1）系统架构设计： 采用前后端分离架构，构建了包含“用户交互层、安全控制层、模型生成层”的三层体系结构。

（2）闭环防御机制： 实现了“预检—干预—评测”的安全闭环。系统首先利用 LLM 检测器对用户输入进行风险筛查与拦截；对于合规但涉及版权风险的请求，调用经过 Latent-DPO 微调的生成模型输出无侵权视频；最后结合 VBench 等工具对生成结果进行后验质量评估。

（3）场景验证： 在恶意攻击拦截、版权 IP 保护等典型应用场景下对系统进行了功能测试与性能评估，验证了该防御体系在实际应用中的稳定性与实用价值。

\section{论文组织结构}

本文围绕文生视频模型的安全防御问题，从数据资源构建、前置安全检测、模型概念擦除及系统集成应用四个维度展开研究，全文共分为六章，各章节的具体内容安排如下：

第一章 绪论。 阐述了文生视频技术的发展背景及其面临的版权侵权、有害内容生成等安全挑战，分析了现有基于图像的防御手段在视频任务中的局限性 。在此基础上，明确了本文的研究意义、主要研究内容及创新点，并给出了论文的总体框架。

第二章 面向视频生成的安全基准与数据资源建设。 针对视频安全研究数据匮乏的问题，详细阐述了相关数据集的构建过程。首先，基于多维度风险分类体系，构建了包含隐喻与复杂攻击样本的多模态安全检测数据集 ；其次，利用冻结的视频扩散模型与自动化筛选流水线，构建了首个无需训练的视频概念擦除基准数据集 VCE-24K ，为后续算法训练与评估奠定了数据基础。

第三章 基于思维链增强的视频生成前置安全检测。 针对输入端恶意提示词的隐蔽性与多义性问题，提出了一种基于大语言模型的前置检测方法。本章详细介绍了如何利用参数高效微调（PEFT）技术将领域安全知识注入 Llama-3.1 模型，并设计了“思维链（Chain-of-Thought）”推理范式与“策略即提示（Policy-as-Prompt）”机制，以提升模型对灰域风险的识别精度与拦截能力。

第四章 基于潜空间偏好优化的视频概念擦除方法。 针对模型端内生风险知识的移除问题，提出了一种高效的 Latent-DPO 概念擦除算法。本章深入推导了在潜空间进行直接偏好优化的理论公式，详细阐述了如何通过构建正负偏好轨迹在不进行像素解码的情况下约束去噪过程。同时，提出了时空-语义对齐损失（STSA），以解决擦除过程中的视频时序崩塌与非目标语义漂移问题。

第五章 文生视频全流程安全防御系统的设计与实现。 基于上述核心算法，设计并实现了一套全流程安全防御原型系统。本章详细描述了系统的总体架构、功能模块设计与关键实现技术，展示了“预检—干预—评测”三阶段闭环的系统工作流程，并通过恶意攻击拦截与版权 IP 保护等典型场景测试了系统的有效性与稳定性。

第六章 总结与展望。 对全文的研究工作与核心成果进行了总结，客观分析了当前研究存在的不足之处，并对未来的研究方向进行了展望。